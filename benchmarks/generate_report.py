#!/usr/bin/env python3
"""
BitNet Full Report Generator
============================
Genera un reporte Markdown con todos los resultados de las pruebas.
Ejecuta cada prueba mÃºltiples veces para medir precisiÃ³n real.

Uso:
    python generate_report.py [--url URL] [--output report.md] [--runs N]
    python generate_report.py --rag  # Usar RAG con auto-learn
    python generate_report.py --compare  # Comparar LLM directo vs RAG inteligente
"""

import os
import sys
import json
import time
import re
import argparse
from datetime import datetime
from dataclasses import dataclass, field
from typing import Optional, List, Dict

try:
    import requests
except ImportError:
    print("âŒ Instala requests: pip install requests")
    exit(1)

DEFAULT_URL = "http://localhost:11435"
DEFAULT_RUNS = 10  # NÃºmero de veces que se ejecuta cada test

# =============================================================================
# System Prompts especÃ­ficos por categorÃ­a (simples, sin conocimiento previo)
# =============================================================================

SYSTEM_CHAT = """Eres un asistente conversacional. Responde de forma natural y amigable.
Responde en el mismo idioma que el usuario."""

SYSTEM_MATH = """Eres una calculadora. Responde SOLO con el nÃºmero resultante.
No expliques, no muestres pasos. Solo el nÃºmero."""

SYSTEM_CODE = """Eres un programador experto. Responde SOLO con cÃ³digo.
No expliques, no agregues comentarios innecesarios. Solo cÃ³digo funcional."""

SYSTEM_TOOLS = """Eres un asistente con acceso a herramientas.

HERRAMIENTAS:
- get_weather(location): Clima de una ciudad
- calculate(expression): Calcular expresiÃ³n matemÃ¡tica
- translate(text, to_language): Traducir texto

RESPONDE SOLO CON JSON: {"tool": "nombre", "arguments": {"param": "valor"}}"""

SYSTEM_REASONING = """Eres un asistente de razonamiento lÃ³gico.
Responde SOLO con la conclusiÃ³n, sin repetir el problema.
SÃ© breve y directo."""

SYSTEM_GENERAL = """Eres un asistente de IA Ãºtil y preciso.
Responde de forma clara y concisa."""

# =============================================================================
# Definiciones
# =============================================================================

@dataclass
class TestCase:
    name: str
    category: str
    system: str
    user: str
    max_tokens: int
    expected: str

@dataclass
class TestResult:
    name: str
    category: str
    runs: int
    passed: int
    accuracy: float
    avg_time_ms: int
    avg_tokens: float
    avg_tps: float
    responses: List[str] = field(default_factory=list)
    expected: str = ""

# =============================================================================
# Tests por categorÃ­a
# =============================================================================

def make_tests():
    return [
        # === CHAT ===
        TestCase("Saludo", "Chat", SYSTEM_CHAT,
                 "Â¡Hola! Â¿CÃ³mo estÃ¡s?",
                 50, "hola/bien/ayud"),
        TestCase("Capital Francia", "Chat", SYSTEM_CHAT,
                 "Â¿CuÃ¡l es la capital de Francia?",
                 30, "ParÃ­s/Paris"),
        TestCase("Capital EspaÃ±a", "Chat", SYSTEM_CHAT,
                 "Â¿CuÃ¡l es la capital de EspaÃ±a?",
                 30, "Madrid"),
        TestCase("QuiÃ©n es Einstein", "Chat", SYSTEM_CHAT,
                 "Â¿QuiÃ©n fue Albert Einstein?",
                 100, "fÃ­sico/cientÃ­fico/physicist/ciencia"),
        
        # === MATEMÃTICAS ===
        TestCase("25+17", "MatemÃ¡ticas", SYSTEM_MATH,
                 "25+17", 20, "42"),
        TestCase("12*11", "MatemÃ¡ticas", SYSTEM_MATH,
                 "12*11", 20, "132"),
        TestCase("100/4", "MatemÃ¡ticas", SYSTEM_MATH,
                 "100/4", 20, "25"),
        TestCase("7^2", "MatemÃ¡ticas", SYSTEM_MATH,
                 "7 al cuadrado", 20, "49"),
        
        # === CÃ“DIGO ===
        TestCase("Hola Mundo", "CÃ³digo", SYSTEM_CODE,
                 "print Hola Mundo en Python",
                 50, "print"),
        TestCase("FunciÃ³n suma", "CÃ³digo", SYSTEM_CODE,
                 "funciÃ³n Python que sume dos nÃºmeros",
                 100, "def"),
        TestCase("Lista reversa", "CÃ³digo", SYSTEM_CODE,
                 "cÃ³digo Python para invertir una lista",
                 100, "reverse/::-1/reversed"),
        TestCase("Bucle for", "CÃ³digo", SYSTEM_CODE,
                 "bucle for en Python del 1 al 5",
                 100, "for"),
        
        # === TOOLS ===
        TestCase("Tool: Clima", "Tools", SYSTEM_TOOLS,
                 "clima en Tokio",
                 150, "get_weather"),
        TestCase("Tool: Calcular", "Tools", SYSTEM_TOOLS,
                 "calcula 25*4",
                 150, "calculate"),
        TestCase("Tool: Traducir", "Tools", SYSTEM_TOOLS,
                 "traduce 'hola' al inglÃ©s",
                 150, "translate"),
        
        # === RAZONAMIENTO ===
        TestCase("Secuencia", "Razonamiento", SYSTEM_REASONING,
                 "Â¿QuÃ© nÃºmero sigue: 2, 4, 6, 8, ?",
                 50, "10"),
        TestCase("Silogismo", "Razonamiento", SYSTEM_REASONING,
                 "Si todos los gatos son animales, y Michi es un gato, Â¿quÃ© es Michi?",
                 50, "animal/gato"),
        TestCase("LÃ³gica", "Razonamiento", SYSTEM_REASONING,
                 "Si llueve, el suelo se moja. EstÃ¡ lloviendo. Â¿CÃ³mo estÃ¡ el suelo?",
                 50, "mojado/wet/moja/hÃºmedo"),
        
        # === GENERAL ===
        TestCase("Saludo formal", "General", SYSTEM_GENERAL,
                 "Buenos dÃ­as",
                 50, "buenos/dÃ­as/hola/salud"),
        TestCase("Despedida", "General", SYSTEM_GENERAL,
                 "AdiÃ³s, gracias por tu ayuda",
                 50, "adiÃ³s/hasta/gusto/nada"),
    ]

# =============================================================================
# Validadores
# =============================================================================

def validate(response: str, expected: str, category: str) -> bool:
    r = response.lower()
    
    if category == "Tools":
        # Buscar tool call en JSON - aceptar variaciones del nombre
        tool_match = re.search(r'"tool"\s*:\s*"([^"]+)"', response)
        if tool_match:
            tool_found = tool_match.group(1).lower()
            # Aceptar variaciones comunes
            expected_variations = {
                "get_weather": ["get_weather", "weather", "getweather"],
                "calculate": ["calculate", "calculator", "calc"],
                "translate": ["translate", "translator", "translation"],
            }
            expected_lower = expected.lower()
            valid_names = expected_variations.get(expected_lower, [expected_lower])
            return tool_found in valid_names
        return False
    
    # Para otros, buscar palabras clave
    keywords = [kw.strip().lower() for kw in expected.split('/')]
    return any(kw in r for kw in keywords)

# =============================================================================
# Sistema RAG (importaciÃ³n lazy)
# =============================================================================

_rag_system = None

def get_rag_system(url: str):
    """Obtiene o crea el sistema RAG con auto-learn"""
    global _rag_system
    if _rag_system is None:
        # AÃ±adir directorio de scripts al path
        scripts_dir = os.path.join(os.path.dirname(__file__), '..', 'scripts')
        sys.path.insert(0, scripts_dir)
        
        from rag import RAGSystem
        _rag_system = RAGSystem(llm_url=url, auto_learn=True)
        print("ğŸ§  Sistema RAG con auto-learn inicializado")
    return _rag_system

# =============================================================================
# Sistema RAG Inteligente (nuevo servidor persistente)
# =============================================================================

RAG_SERVER_URL = os.getenv("RAG_SERVER_URL", "http://localhost:11436")

def run_single_test_smart_rag(url: str, test: TestCase) -> tuple:
    """Ejecuta un solo test usando el RAG Server inteligente"""
    try:
        start = time.time()
        
        # El RAG server clasifica automÃ¡ticamente y decide la estrategia
        response = requests.post(
            f"{RAG_SERVER_URL}/query",
            json={
                "question": test.user,
                "user_id": "benchmark"
            },
            timeout=120
        )
        response.raise_for_status()
        data = response.json()
        
        elapsed = time.time() - start
        content = data.get("answer", "")
        
        # Obtener info de clasificaciÃ³n
        classification = data.get("classification", {})
        timing = data.get("timing", {})
        
        # Estimar tokens (el RAG server no devuelve esto directamente)
        tokens = len(content.split()) * 1.3
        tps = tokens / elapsed if elapsed > 0 else 0
        
        passed = validate(content, test.expected, test.category)
        
        # AÃ±adir info de estrategia usada
        strategy = classification.get("strategy", "unknown")
        response_info = f"[{strategy}] {content[:180]}"
        
        return (passed, int(elapsed * 1000), int(tokens), tps, response_info)
    except requests.exceptions.ConnectionError:
        return (False, 0, 0, 0, "Error: RAG Server no disponible")
    except Exception as e:
        return (False, 0, 0, 0, f"Error RAG Smart: {e}")

# =============================================================================
# Runner con mÃºltiples ejecuciones
# =============================================================================

def run_single_test_rag(url: str, test: TestCase) -> tuple:
    """Ejecuta un solo test usando RAG con auto-learn"""
    try:
        rag = get_rag_system(url)
        start = time.time()
        
        # CategorÃ­as que requieren system prompt especÃ­fico (no buscar en web)
        # Chat y General usan RAG con bÃºsqueda web, el resto va directo al LLM
        direct_categories = ["MatemÃ¡ticas", "CÃ³digo", "Tools", "Razonamiento"]
        
        if test.category in direct_categories:
            # Bypass RAG: ir directo al LLM con el system prompt original
            system_prompt = test.system
        else:
            # Usar RAG con bÃºsqueda web para conocimiento general
            system_prompt = None
        
        # Usar RAG query (que incluye auto-learn con bÃºsqueda web si system_prompt=None)
        response = rag.query(test.user, stream=False, system_prompt=system_prompt)
        
        elapsed = time.time() - start
        # El RAG no da tokens directamente, estimamos
        tokens = len(response.split()) * 1.3  # Aprox tokens por palabras
        tps = tokens / elapsed if elapsed > 0 else 0
        
        passed = validate(response, test.expected, test.category)
        
        return (passed, int(elapsed * 1000), int(tokens), tps, response[:200])
    except Exception as e:
        return (False, 0, 0, 0, f"Error RAG: {e}")

def run_single_test(session, url: str, test: TestCase) -> tuple:
    """Ejecuta un solo test y retorna (passed, time_ms, tokens, tps, response)"""
    try:
        start = time.time()
        r = session.post(
            f"{url}/v1/chat/completions",
            json={
                "model": "bitnet",
                "messages": [
                    {"role": "system", "content": test.system},
                    {"role": "user", "content": test.user}
                ],
                "max_tokens": test.max_tokens,
                "temperature": 0.3
            },
            timeout=60
        )
        elapsed = time.time() - start
        
        data = r.json()
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
        tokens = data.get("usage", {}).get("completion_tokens", 0)
        tps = tokens / elapsed if elapsed > 0 else 0
        
        passed = validate(content, test.expected, test.category)
        
        return (passed, int(elapsed * 1000), tokens, tps, content[:200])
    except Exception as e:
        return (False, 0, 0, 0, f"Error: {e}")

def run_tests(url: str, runs: int = DEFAULT_RUNS, use_rag: bool = False, 
              use_smart_rag: bool = False, category_filter: str = None) -> list:
    """Ejecuta cada test mÃºltiples veces y calcula precisiÃ³n"""
    results = []
    all_tests = make_tests()
    
    # Filtrar por categorÃ­a si se especifica
    if category_filter:
        tests = [t for t in all_tests if t.category.lower() == category_filter.lower()]
        if not tests:
            print(f"âŒ CategorÃ­a '{category_filter}' no encontrada.")
            print(f"   CategorÃ­as disponibles: {', '.join(set(t.category for t in all_tests))}")
            return []
    else:
        tests = all_tests
    
    session = requests.Session() if not (use_rag or use_smart_rag) else None
    
    total_runs = len(tests) * runs
    current = 0
    
    if use_smart_rag:
        mode_str = "RAG Inteligente ğŸ§ âœ¨"
    elif use_rag:
        mode_str = "RAG auto-learn ğŸ§ "
    else:
        mode_str = "LLM directo"
    
    cat_str = f" [{category_filter}]" if category_filter else ""
    print(f"\nğŸ“Š Ejecutando {len(tests)} tests{cat_str} Ã— {runs} veces = {total_runs} ejecuciones ({mode_str})\n")
    
    for test in tests:
        passed_count = 0
        times = []
        tokens_list = []
        tps_list = []
        responses = []
        
        print(f"[{test.name}] ", end="", flush=True)
        
        for run in range(runs):
            current += 1
            
            if use_smart_rag:
                passed, time_ms, tokens, tps, response = run_single_test_smart_rag(url, test)
            elif use_rag:
                passed, time_ms, tokens, tps, response = run_single_test_rag(url, test)
            else:
                passed, time_ms, tokens, tps, response = run_single_test(session, url, test)
            
            if passed:
                passed_count += 1
                print("âœ“", end="", flush=True)
            else:
                print("âœ—", end="", flush=True)
            
            times.append(time_ms)
            tokens_list.append(tokens)
            tps_list.append(tps)
            responses.append(response)
        
        accuracy = (passed_count / runs) * 100
        avg_time = sum(times) / len(times) if times else 0
        avg_tokens = sum(tokens_list) / len(tokens_list) if tokens_list else 0
        avg_tps = sum(tps_list) / len(tps_list) if tps_list else 0
        
        print(f" â†’ {accuracy:.0f}% ({passed_count}/{runs})")
        
        results.append(TestResult(
            name=test.name,
            category=test.category,
            runs=runs,
            passed=passed_count,
            accuracy=accuracy,
            avg_time_ms=int(avg_time),
            avg_tokens=round(avg_tokens, 1),
            avg_tps=round(avg_tps, 1),
            responses=responses[:3],  # Guardar solo 3 ejemplos
            expected=test.expected
        ))
    
    return results

# =============================================================================
# Markdown Report
# =============================================================================

def generate_markdown(results: list, url: str, runs: int) -> str:
    total_tests = len(results)
    total_runs = total_tests * runs
    total_passed = sum(r.passed for r in results)
    overall_accuracy = (total_passed / total_runs) * 100 if total_runs > 0 else 0
    total_time = sum(r.avg_time_ms * runs for r in results)
    total_tokens = sum(r.avg_tokens * runs for r in results)
    avg_tps = sum(r.avg_tps for r in results) / len(results) if results else 0
    
    # Tests con 100% accuracy
    perfect_tests = sum(1 for r in results if r.accuracy == 100)
    
    # Agrupar por categorÃ­a
    categories = {}
    for r in results:
        if r.category not in categories:
            categories[r.category] = {"tests": 0, "passed": 0, "runs": 0, "time": 0}
        categories[r.category]["tests"] += 1
        categories[r.category]["passed"] += r.passed
        categories[r.category]["runs"] += r.runs
        categories[r.category]["time"] += r.avg_time_ms
    
    # Construir markdown
    md = f"""# ğŸ§ª BitNet Benchmark Report - AnÃ¡lisis de PrecisiÃ³n

**Fecha:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Server:** {url}  
**Ejecuciones:** Cada test ejecutado {runs} veces para medir consistencia

---

## ğŸ“Š Resumen Global

| MÃ©trica | Valor |
|---------|-------|
| **PrecisiÃ³n Global** | **{overall_accuracy:.1f}%** |
| Tests Totales | {total_tests} |
| Ejecuciones | {total_runs} |
| Tests con 100% âœ“ | {perfect_tests} |
| Tiempo Total | {total_time/1000:.1f}s |
| Velocidad Promedio | {avg_tps:.1f} t/s |

---

## ğŸ“Š PrecisiÃ³n por CategorÃ­a

| CategorÃ­a | Tests | Pasaron | PrecisiÃ³n | Tiempo Prom. |
|-----------|-------|---------|-----------|--------------|
"""
    
    # Agregar filas de categorÃ­as
    for cat, stats in sorted(categories.items()):
        pct = (stats["passed"] / stats["runs"] * 100) if stats["runs"] > 0 else 0
        avg_time = stats["time"] / stats["tests"] if stats["tests"] > 0 else 0
        
        # Emoji segÃºn precisiÃ³n
        if pct == 100:
            emoji = "ğŸ¯"
        elif pct >= 70:
            emoji = "âœ…"
        elif pct >= 40:
            emoji = "âš ï¸"
        else:
            emoji = "âŒ"
        
        md += f"| {cat} | {stats['tests']} | {stats['passed']}/{stats['runs']} | {emoji} **{pct:.1f}%** | {avg_time:.0f}ms |\n"
    
    md += "\n---\n\n## ğŸ“‹ Resultados Detallados\n\n"
    md += "**Leyenda:** ğŸ¯ 100% | âœ… â‰¥70% | âš ï¸ â‰¥40% | âŒ <40%\n\n"
    md += "| Test | CategorÃ­a | PrecisiÃ³n | Pasaron | Tiempo | Tokens | T/s | Esperado |\n"
    md += "|------|-----------|-----------|---------|--------|--------|-----|----------|\n"
    
    # Agregar filas de resultados
    for r in results:
        # Emoji segÃºn precisiÃ³n
        if r.accuracy == 100:
            emoji = "ğŸ¯"
        elif r.accuracy >= 70:
            emoji = "âœ…"
        elif r.accuracy >= 40:
            emoji = "âš ï¸"
        else:
            emoji = "âŒ"
        
        md += f"| {r.name} | {r.category} | {emoji} **{r.accuracy:.0f}%** | {r.passed}/{r.runs} | {r.avg_time_ms}ms | {r.avg_tokens} | {r.avg_tps} | `{r.expected}` |\n"
    
    md += "\n---\n\n## ğŸ“ Muestras de Respuestas\n\n"
    
    # Agregar muestras de respuestas (solo las primeras 5)
    for i, r in enumerate(results[:5], 1):
        md += f"### {i}. {r.name} ({r.category})\n\n"
        md += f"**Esperado:** `{r.expected}`  \n"
        md += f"**PrecisiÃ³n:** {r.accuracy:.0f}%\n\n"
        
        if r.responses:
            md += "**Ejemplo de respuesta:**\n```\n"
            sample = r.responses[0][:200]
            if len(r.responses[0]) > 200:
                sample += "..."
            md += sample + "\n```\n\n"
    
    md += "---\n\n"
    md += f"*Generado por neuro-bitnet benchmark suite*  \n"
    md += f"*Cada test se ejecutÃ³ {runs} veces para medir precisiÃ³n estadÃ­stica*\n"
    
    return md

# =============================================================================
# Reporte Comparativo (LLM vs RAG)
# =============================================================================

def generate_comparison_markdown(results_llm: list, results_rag: list, url: str, runs: int) -> str:
    """Genera reporte comparando LLM directo vs RAG inteligente"""
    
    # Calcular mÃ©tricas para LLM
    total_tests = len(results_llm)
    total_runs = total_tests * runs
    
    llm_passed = sum(r.passed for r in results_llm)
    llm_accuracy = (llm_passed / total_runs) * 100 if total_runs > 0 else 0
    llm_time = sum(r.avg_time_ms for r in results_llm) / len(results_llm) if results_llm else 0
    llm_tps = sum(r.avg_tps for r in results_llm) / len(results_llm) if results_llm else 0
    
    # Calcular mÃ©tricas para RAG
    rag_passed = sum(r.passed for r in results_rag)
    rag_accuracy = (rag_passed / total_runs) * 100 if total_runs > 0 else 0
    rag_time = sum(r.avg_time_ms for r in results_rag) / len(results_rag) if results_rag else 0
    rag_tps = sum(r.avg_tps for r in results_rag) / len(results_rag) if results_rag else 0
    
    # Mejora
    accuracy_diff = rag_accuracy - llm_accuracy
    accuracy_emoji = "ğŸ“ˆ" if accuracy_diff > 0 else ("ğŸ“‰" if accuracy_diff < 0 else "â¡ï¸")
    
    md = f"""# ğŸ§ª BitNet Benchmark Report - ComparaciÃ³n LLM vs RAG Inteligente

**Fecha:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Server LLM:** {url}  
**Server RAG:** {RAG_SERVER_URL}  
**Ejecuciones:** Cada test ejecutado {runs} veces

---

## ğŸ“Š Resumen Comparativo

| MÃ©trica | ğŸ”µ LLM Directo | ğŸŸ¢ RAG Inteligente | Diferencia |
|---------|---------------|-------------------|------------|
| **PrecisiÃ³n Global** | **{llm_accuracy:.1f}%** | **{rag_accuracy:.1f}%** | {accuracy_emoji} **{accuracy_diff:+.1f}%** |
| Tests Pasados | {llm_passed}/{total_runs} | {rag_passed}/{total_runs} | {rag_passed - llm_passed:+d} |
| Tiempo Promedio | {llm_time:.0f}ms | {rag_time:.0f}ms | {rag_time - llm_time:+.0f}ms |
| Velocidad (t/s) | {llm_tps:.1f} | {rag_tps:.1f} | {rag_tps - llm_tps:+.1f} |

---

## ğŸ“Š ComparaciÃ³n por CategorÃ­a

| CategorÃ­a | ğŸ”µ LLM | ğŸŸ¢ RAG | Mejora | Estrategia RAG |
|-----------|--------|--------|--------|----------------|
"""
    
    # Agrupar por categorÃ­a
    categories_llm = {}
    categories_rag = {}
    
    for r in results_llm:
        if r.category not in categories_llm:
            categories_llm[r.category] = {"passed": 0, "runs": 0}
        categories_llm[r.category]["passed"] += r.passed
        categories_llm[r.category]["runs"] += r.runs
    
    for r in results_rag:
        if r.category not in categories_rag:
            categories_rag[r.category] = {"passed": 0, "runs": 0}
        categories_rag[r.category]["passed"] += r.passed
        categories_rag[r.category]["runs"] += r.runs
    
    # Estrategias esperadas por categorÃ­a
    expected_strategies = {
        "Chat": "RAG â†’ Web (factual)",
        "MatemÃ¡ticas": "LLM Directo",
        "CÃ³digo": "LLM Directo",
        "Tools": "LLM Directo",
        "Razonamiento": "LLM Directo",
        "General": "LLM Directo (saludos)",
    }
    
    for cat in sorted(categories_llm.keys()):
        llm_stats = categories_llm.get(cat, {"passed": 0, "runs": 1})
        rag_stats = categories_rag.get(cat, {"passed": 0, "runs": 1})
        
        llm_pct = (llm_stats["passed"] / llm_stats["runs"] * 100) if llm_stats["runs"] > 0 else 0
        rag_pct = (rag_stats["passed"] / rag_stats["runs"] * 100) if rag_stats["runs"] > 0 else 0
        diff = rag_pct - llm_pct
        
        if diff > 5:
            emoji = "ğŸ¯"
        elif diff > 0:
            emoji = "âœ…"
        elif diff == 0:
            emoji = "â¡ï¸"
        else:
            emoji = "âš ï¸"
        
        strategy = expected_strategies.get(cat, "Auto")
        md += f"| {cat} | {llm_pct:.0f}% | {rag_pct:.0f}% | {emoji} {diff:+.0f}% | {strategy} |\n"
    
    md += "\n---\n\n## ğŸ“‹ Resultados Detallados por Test\n\n"
    md += "| Test | CategorÃ­a | ğŸ”µ LLM | ğŸŸ¢ RAG | Mejora |\n"
    md += "|------|-----------|--------|--------|--------|\n"
    
    for i, r_llm in enumerate(results_llm):
        r_rag = results_rag[i] if i < len(results_rag) else None
        
        llm_acc = r_llm.accuracy
        rag_acc = r_rag.accuracy if r_rag else 0
        diff = rag_acc - llm_acc
        
        if diff > 10:
            emoji = "ğŸ¯"
        elif diff > 0:
            emoji = "âœ…"
        elif diff == 0:
            emoji = "â¡ï¸"
        else:
            emoji = "âš ï¸"
        
        md += f"| {r_llm.name} | {r_llm.category} | {llm_acc:.0f}% | {rag_acc:.0f}% | {emoji} {diff:+.0f}% |\n"
    
    # Tests con mayor mejora
    improvements = []
    for i, r_llm in enumerate(results_llm):
        if i < len(results_rag):
            r_rag = results_rag[i]
            diff = r_rag.accuracy - r_llm.accuracy
            if diff > 0:
                improvements.append((r_llm.name, r_llm.category, r_llm.accuracy, r_rag.accuracy, diff))
    
    if improvements:
        improvements.sort(key=lambda x: x[4], reverse=True)
        md += "\n---\n\n## ğŸ¯ Tests con Mayor Mejora usando RAG\n\n"
        md += "| Test | CategorÃ­a | LLM â†’ RAG | Mejora |\n"
        md += "|------|-----------|-----------|--------|\n"
        for name, cat, llm_acc, rag_acc, diff in improvements[:5]:
            md += f"| {name} | {cat} | {llm_acc:.0f}% â†’ {rag_acc:.0f}% | **+{diff:.0f}%** |\n"
    
    # Tests con degradaciÃ³n
    degradations = []
    for i, r_llm in enumerate(results_llm):
        if i < len(results_rag):
            r_rag = results_rag[i]
            diff = r_rag.accuracy - r_llm.accuracy
            if diff < 0:
                degradations.append((r_llm.name, r_llm.category, r_llm.accuracy, r_rag.accuracy, diff))
    
    if degradations:
        degradations.sort(key=lambda x: x[4])
        md += "\n---\n\n## âš ï¸ Tests donde RAG fue Peor\n\n"
        md += "| Test | CategorÃ­a | LLM â†’ RAG | Diferencia |\n"
        md += "|------|-----------|-----------|------------|\n"
        for name, cat, llm_acc, rag_acc, diff in degradations:
            md += f"| {name} | {cat} | {llm_acc:.0f}% â†’ {rag_acc:.0f}% | **{diff:.0f}%** |\n"
    
    # Muestras de respuestas comparativas
    md += "\n---\n\n## ğŸ“ Ejemplos de Respuestas Comparativas\n\n"
    
    # Mostrar tests donde hubo diferencia significativa
    interesting_tests = []
    for i, r_llm in enumerate(results_llm):
        if i < len(results_rag):
            r_rag = results_rag[i]
            diff = abs(r_rag.accuracy - r_llm.accuracy)
            if diff >= 30:  # Diferencia significativa
                interesting_tests.append((r_llm, r_rag, diff))
    
    interesting_tests.sort(key=lambda x: x[2], reverse=True)
    
    for r_llm, r_rag, diff in interesting_tests[:3]:
        md += f"### {r_llm.name} ({r_llm.category})\n\n"
        md += f"**Esperado:** `{r_llm.expected}`\n\n"
        md += f"| Modo | PrecisiÃ³n | Respuesta |\n"
        md += f"|------|-----------|----------|\n"
        
        llm_sample = r_llm.responses[0][:100] if r_llm.responses else "N/A"
        rag_sample = r_rag.responses[0][:100] if r_rag.responses else "N/A"
        
        md += f"| ğŸ”µ LLM | {r_llm.accuracy:.0f}% | `{llm_sample}...` |\n"
        md += f"| ğŸŸ¢ RAG | {r_rag.accuracy:.0f}% | `{rag_sample}...` |\n\n"
    
    # Conclusiones
    md += "\n---\n\n## ğŸ“ˆ Conclusiones\n\n"
    
    if accuracy_diff > 5:
        md += f"âœ… **El RAG Inteligente mejora la precisiÃ³n en {accuracy_diff:.1f}%**\n\n"
        md += "El sistema RAG clasifica las consultas y usa la estrategia Ã³ptima:\n"
        md += "- **Consultas factuales** (capitales, personas, historia): Busca en RAG/Web\n"
        md += "- **MatemÃ¡ticas, cÃ³digo, razonamiento**: Usa LLM directo (mÃ¡s rÃ¡pido)\n"
    elif accuracy_diff < -5:
        md += f"âš ï¸ **El RAG Inteligente tuvo menor precisiÃ³n ({accuracy_diff:.1f}%)**\n\n"
        md += "Posibles causas:\n"
        md += "- InformaciÃ³n desactualizada en la web\n"
        md += "- Contexto RAG introduciendo ruido\n"
    else:
        md += f"â¡ï¸ **PrecisiÃ³n similar entre ambos modos ({accuracy_diff:+.1f}%)**\n\n"
        md += "El RAG aÃ±ade valor en consultas factuales sin degradar las demÃ¡s.\n"
    
    time_diff = rag_time - llm_time
    if time_diff > 500:
        md += f"\nâ±ï¸ **El RAG es {time_diff:.0f}ms mÃ¡s lento en promedio** (debido a bÃºsquedas web)\n"
    elif time_diff < -100:
        md += f"\nâš¡ **El RAG es {-time_diff:.0f}ms mÃ¡s rÃ¡pido** (cachÃ© de embeddings)\n"
    
    md += "\n---\n\n"
    md += f"*Generado por neuro-bitnet benchmark suite*  \n"
    md += f"*ComparaciÃ³n: LLM directo vs RAG Server Inteligente*  \n"
    md += f"*Cada test se ejecutÃ³ {runs} veces*\n"
    
    return md

# =============================================================================
# Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="BitNet Report Generator con anÃ¡lisis de precisiÃ³n")
    parser.add_argument("--url", default=DEFAULT_URL)
    parser.add_argument("--output", "-o", default="benchmark_report.md")
    parser.add_argument("--runs", "-r", type=int, default=DEFAULT_RUNS,
                        help=f"NÃºmero de veces que se ejecuta cada test (default: {DEFAULT_RUNS})")
    parser.add_argument("--rag", action="store_true",
                        help="Usar sistema RAG con auto-learn (bÃºsqueda web cuando no sabe)")
    parser.add_argument("--smart-rag", action="store_true",
                        help="Usar RAG Server inteligente (clasificaciÃ³n automÃ¡tica)")
    parser.add_argument("--compare", action="store_true",
                        help="Comparar LLM directo vs RAG inteligente (genera ambos)")
    parser.add_argument("--rag-url", default=None,
                        help=f"URL del RAG Server (default: {RAG_SERVER_URL})")
    parser.add_argument("--category", "-c", type=str, default=None,
                        help="Filtrar por categorÃ­a: Chat, MatemÃ¡ticas, CÃ³digo, Tools, Razonamiento, General")
    args = parser.parse_args()
    
    # Actualizar URL del RAG server si se especifica
    rag_server_url = args.rag_url if args.rag_url else RAG_SERVER_URL
    
    print(f"\nğŸ§ª BitNet Benchmark - AnÃ¡lisis de PrecisiÃ³n")
    print(f"ğŸ”— URL LLM: {args.url}")
    if args.compare or args.smart_rag:
        print(f"ğŸ”— URL RAG: {rag_server_url}")
    print(f"ğŸ”„ Runs por test: {args.runs}")
    
    if args.compare:
        print(f"ğŸ“Š Modo: ComparaciÃ³n LLM vs RAG Inteligente")
    elif args.smart_rag:
        print(f"ğŸ§  Modo: RAG Server inteligente")
    elif args.rag:
        print(f"ğŸ§  Modo: RAG con auto-learn")
    else:
        print(f"ğŸ’¬ Modo: LLM directo")
    
    if args.category:
        print(f"ğŸ“‚ CategorÃ­a: {args.category}")
    print("=" * 60)
    
    # Verificar servidor LLM
    try:
        r = requests.get(f"{args.url}/health", timeout=5)
        if r.status_code != 200:
            raise Exception("Health check failed")
        print("âœ… Servidor LLM disponible")
    except:
        print(f"âŒ No se puede conectar al LLM en {args.url}")
        exit(1)
    
    # Verificar RAG Server si es necesario
    if args.compare or args.smart_rag:
        try:
            r = requests.get(f"{rag_server_url}/health", timeout=5)
            if r.status_code != 200:
                raise Exception("RAG Health check failed")
            print("âœ… RAG Server disponible")
        except:
            print(f"âŒ No se puede conectar al RAG Server en {rag_server_url}")
            print(f"   Inicia el servidor: python scripts/rag_server.py")
            exit(1)
    
    # Modo comparaciÃ³n: ejecutar ambos
    if args.compare:
        print("\n" + "=" * 60)
        print("ğŸ”µ FASE 1: Tests con LLM Directo")
        print("=" * 60)
        results_llm = run_tests(args.url, args.runs, use_rag=False, 
                                use_smart_rag=False, category_filter=args.category)
        
        print("\n" + "=" * 60)
        print("ğŸŸ¢ FASE 2: Tests con RAG Inteligente")
        print("=" * 60)
        results_rag = run_tests(args.url, args.runs, use_rag=False,
                                use_smart_rag=True, category_filter=args.category)
        
        if not results_llm or not results_rag:
            exit(1)
        
        # Generar reporte comparativo
        markdown = generate_comparison_markdown(results_llm, results_rag, args.url, args.runs)
        
        # Calcular mÃ©tricas finales
        llm_passed = sum(r.passed for r in results_llm)
        rag_passed = sum(r.passed for r in results_rag)
        total_runs = sum(r.runs for r in results_llm)
        
        llm_accuracy = (llm_passed / total_runs) * 100 if total_runs > 0 else 0
        rag_accuracy = (rag_passed / total_runs) * 100 if total_runs > 0 else 0
        diff = rag_accuracy - llm_accuracy
        
        print("\n" + "=" * 60)
        print("ğŸ“Š COMPARACIÃ“N FINAL")
        print("=" * 60)
        print(f"ğŸ”µ LLM Directo:      {llm_accuracy:.1f}%")
        print(f"ğŸŸ¢ RAG Inteligente:  {rag_accuracy:.1f}%")
        print(f"ğŸ“ˆ Diferencia:       {diff:+.1f}%")
        
    else:
        # Modo simple: un solo tipo de test
        results = run_tests(args.url, args.runs, use_rag=args.rag,
                           use_smart_rag=args.smart_rag, category_filter=args.category)
        
        if not results:
            exit(1)
        
        # Generar reporte
        markdown = generate_markdown(results, args.url, args.runs)
        
        # Resumen
        total_passed = sum(r.passed for r in results)
        total_runs = sum(r.runs for r in results)
        overall_accuracy = (total_passed / total_runs) * 100 if total_runs > 0 else 0
        perfect = sum(1 for r in results if r.accuracy == 100)
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š PRECISIÃ“N GLOBAL: {overall_accuracy:.1f}%")
        print(f"ğŸ¯ Tests con 100%: {perfect}/{len(results)}")
    
    # Guardar reporte
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(markdown)
    
    print(f"ğŸ“„ Reporte generado: {args.output}")
    print("=" * 60)

if __name__ == "__main__":
    main()