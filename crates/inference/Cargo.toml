[package]
name = "neuro-inference"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
description = "Local LLM inference using bitnet.cpp for BitNet 1.58-bit models"

[dependencies]
# Async runtime for subprocess and downloads  
tokio = { version = "1.0", features = ["rt-multi-thread", "fs", "process", "io-util"] }

# HTTP client for model downloads and translation API
reqwest = { version = "0.12", default-features = false, features = ["rustls-tls", "stream", "json"] }
futures-util = { version = "0.3", optional = true }
urlencoding = "2.1"

# Progress bar and checksums
indicatif = { version = "0.17", optional = true }
sha2 = { version = "0.10", optional = true }

# Common dependencies
thiserror = "1.0"
tracing = "0.1"
encoding_rs = "0.8"
rand = "0.8"
serde = { version = "1.0", features = ["derive"] }
dirs = "5.0"
once_cell = "1.19"

# Native bindings (optional)
bitnet-sys = { path = "../bitnet-sys", optional = true }
crossbeam-channel = { version = "0.5", optional = true }
num_cpus = { version = "1.16", optional = true }
regex.workspace = true

[features]
default = ["subprocess", "download"]
# Subprocess backend (calls llama-cli binary)
subprocess = []
# Native FFI bindings to bitnet.cpp (fastest)
native = ["dep:bitnet-sys", "dep:crossbeam-channel", "dep:num_cpus"]
# CUDA GPU acceleration (requires native)
cuda = ["native", "bitnet-sys/cuda"]
# Model download support with progress bars
download = ["dep:futures-util", "dep:indicatif", "dep:sha2"]

